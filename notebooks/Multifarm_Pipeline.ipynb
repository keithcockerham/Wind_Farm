{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "105feb2f-b8fd-4c1a-b58d-2c826151781c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import psutil\n",
    "import sys\n",
    "import re\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "954604f1-eb0f-4666-bab0-1132920f91e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = r\"D:\\Data\\SCADA\\Wind_Turbine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a26f9353-1d01-4155-98e8-8d9e6f89ccad",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_event_info(farm='A'):\n",
    "    farm_dir = os.path.join(BASE_DIR, 'Wind Farm '+farm)\n",
    "    event_info = pd.read_csv(farm_dir + '\\\\event_info.csv', sep=';')\n",
    "    event_info = event_info.rename(columns={'asset':'asset_id'})\n",
    "    # Drop events that aren't anomalies\n",
    "    event_info = event_info[event_info['event_label'] == 'anomaly']\n",
    "    # Clean Up\n",
    "    event_info[\"event_start\"] = pd.to_datetime(event_info[\"event_start\"])\n",
    "    event_info[\"event_end\"] = pd.to_datetime(event_info[\"event_end\"])\n",
    "    event_info[\"asset_id\"] = pd.to_numeric(event_info[\"asset_id\"], errors=\"coerce\").astype(\"Int16\")\n",
    "    # Drop rows with missing critical info\n",
    "    event_info = event_info.dropna(subset=[\"asset_id\", \"event_start\", 'event_end'])\n",
    "    # Sort for asset_id\n",
    "    event_info = event_info.sort_values([\"asset_id\"]).reset_index(drop=True)\n",
    "\n",
    "    return event_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c30f9eec-f6fe-4e37-9d83-54625be275ce",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_farm_scada_chunked(farm='A', asset_ids=None, chunksize=50000):\n",
    "    \"\"\"\n",
    "    Process farm data in chunks - for when dataset won't fit in RAM.\n",
    "    \n",
    "    asset_ids: Process only specific assets (e.g., [0, 10, 21])\n",
    "    chunksize: Rows per chunk\n",
    "    \"\"\"\n",
    "    farm_dir = os.path.join(BASE_DIR, 'Wind Farm '+farm)\n",
    "    farm_dataset_dir = os.path.join(farm_dir, 'datasets')\n",
    "    all_files = glob.glob(os.path.join(farm_dataset_dir, '*.csv'))\n",
    "    parquet_path = os.path.join(farm_dir, f'farm_{farm}_optimized.parquet')\n",
    "    dtype_dict = {\n",
    "        'asset_id': 'int16',\n",
    "        'status_type_id': 'int8',\n",
    "    }\n",
    "    # Check if already processed\n",
    "    if os.path.exists(parquet_path):\n",
    "        print(f\"Loading pre-processed Farm {farm}...\")\n",
    "        return pd.read_parquet(parquet_path)\n",
    "        \n",
    "    all_data = []\n",
    "    \n",
    "    for f in all_files:\n",
    "        # Read in chunks\n",
    "        for chunk in pd.read_csv(f, sep=\";\", dtype=dtype_dict, chunksize=chunksize):\n",
    "            # Filter to specific assets if provided\n",
    "            if asset_ids is not None:\n",
    "                chunk = chunk[chunk['asset_id'].isin(asset_ids)]\n",
    "            \n",
    "            # Optimize dtypes\n",
    "            float_cols = chunk.select_dtypes(include=['float64']).columns\n",
    "            chunk[float_cols] = chunk[float_cols].astype('float32')\n",
    "            \n",
    "            # Clean\n",
    "            chunk[\"time_stamp\"] = pd.to_datetime(chunk[\"time_stamp\"])\n",
    "            chunk = chunk.drop(['train_test', 'id'], axis=1, errors='ignore')\n",
    "            chunk = chunk.dropna(subset=[\"asset_id\", \"time_stamp\"])\n",
    "            \n",
    "            all_data.append(chunk)\n",
    "    \n",
    "    scada_data = pd.concat(all_data, ignore_index=True)\n",
    "    scada_data = scada_data.drop_duplicates(subset=['asset_id', 'time_stamp'], keep='first')\n",
    "    scada_data = scada_data.sort_values([\"asset_id\", \"time_stamp\"]).reset_index(drop=True)\n",
    "\n",
    "    scada_data.to_parquet(parquet_path, compression='snappy', index=False)\n",
    "    print(f\"Saved to {parquet_path}\")\n",
    "    \n",
    "    return scada_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1bd3f53-ab65-42b0-8c04-a1dcea21ecd5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class WindTurbineFailurePipeline:\n",
    "    \"\"\"\n",
    "    Reproducible pipeline for wind turbine failure prediction from SCADA data.\n",
    "    Implements methodology developed on Wind Farm A.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, power_sensor, power_threshold=0.1, buffer_days=30, window_hours=24):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        power_threshold : float\n",
    "            Minimum power for \"actual production\" (default 0.1)\n",
    "        buffer_days : int\n",
    "            Exclusion buffer around failures for normal baseline (default 30)\n",
    "        window_hours : int\n",
    "            Prediction window length in hours (default 24)\n",
    "        \"\"\"\n",
    "        self.power_threshold = power_threshold\n",
    "        self.power_sensor = power_sensor\n",
    "        self.buffer_days = buffer_days\n",
    "        self.window_hours = window_hours\n",
    "        self.selected_features = None\n",
    "        self.feature_importance = None\n",
    "        \n",
    "    def validate_data_structure(self, scada, event_info):\n",
    "        \"\"\"\n",
    "        Step 1: Validate data has required columns and structure\n",
    "        \"\"\"\n",
    "        print(\"STEP 1: DATA VALIDATION\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Check SCADA columns\n",
    "        required_scada = ['time_stamp', 'asset_id', 'status_type_id']\n",
    "        missing_scada = [col for col in required_scada if col not in scada.columns]\n",
    "        \n",
    "        if missing_scada:\n",
    "            raise ValueError(f\"SCADA missing columns: {missing_scada}\")\n",
    "            \n",
    "        # Check event_info columns\n",
    "        required_events = ['event_id', 'asset_id', 'event_start', 'event_end', 'event_label']\n",
    "        missing_events = [col for col in required_events if col not in event_info.columns]\n",
    "        \n",
    "        if missing_events:\n",
    "            raise ValueError(f\"Event info missing columns: {missing_events}\")\n",
    "        \n",
    "        # Find power column (could be power_30_avg, power_avg, etc.)\n",
    "        power_cols = [col for col in scada.columns if 'power' in col.lower() and 'avg' in col.lower()]\n",
    "        if not power_cols:\n",
    "            raise ValueError(\"No power column found in SCADA data\")\n",
    "        \n",
    "        self.power_column = power_cols[0]  # Use first power column found\n",
    "        \n",
    "        print(f\"✓ Data structure valid\")\n",
    "        print(f\"  SCADA records: {len(scada):,}\")\n",
    "        print(f\"  Unique assets: {scada['asset_id'].nunique()}\")\n",
    "        print(f\"  Total events: {len(event_info)}\")\n",
    "        print(f\"  Anomaly events: {(event_info['event_label']=='anomaly').sum()}\")\n",
    "        print(f\"  Power column: {self.power_column}\")\n",
    "        print(f\"  Status values: {sorted(scada['status_type_id'].unique())}\")\n",
    "        \n",
    "    def analyze_temporal_patterns(self, scada, event_info):\n",
    "        \"\"\"\n",
    "        Step 2: Analyze time gaps between last production and logged failures\n",
    "        \"\"\"\n",
    "        print(\"\\nSTEP 2: TEMPORAL PATTERN ANALYSIS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        gaps = []\n",
    "        for _, failure in event_info[event_info['event_label']=='anomaly'].iterrows():\n",
    "            asset_id = failure['asset_id']\n",
    "            failure_start = failure['event_start']\n",
    "            \n",
    "            production_data = scada[\n",
    "                (scada['asset_id'] == asset_id) &\n",
    "                (scada['time_stamp'] < failure_start) &\n",
    "                (scada['status_type_id'] == 0) &\n",
    "                (scada[self.power_column] > self.power_threshold)\n",
    "            ].sort_values('time_stamp')\n",
    "            \n",
    "            if len(production_data) == 0:\n",
    "                print(f\"  WARNING: Event {failure['event_id']} has no production data before failure\")\n",
    "                continue\n",
    "                \n",
    "            last_production = production_data.iloc[-1]['time_stamp']\n",
    "            gap = failure_start - last_production\n",
    "            gaps.append({\n",
    "                'event_id': failure['event_id'],\n",
    "                'gap_hours': gap.total_seconds() / 3600,\n",
    "                'gap_days': gap.total_seconds() / (3600*24)\n",
    "            })\n",
    "        \n",
    "        gaps_df = pd.DataFrame(gaps)\n",
    "        print(f\"\\n  Gap statistics (hours):\")\n",
    "        print(f\"    Mean: {gaps_df['gap_hours'].mean():.1f}\")\n",
    "        print(f\"    Median: {gaps_df['gap_hours'].median():.1f}\")\n",
    "        print(f\"    Min: {gaps_df['gap_hours'].min():.1f}\")\n",
    "        print(f\"    Max: {gaps_df['gap_hours'].max():.1f}\")\n",
    "        \n",
    "        return gaps_df\n",
    "        \n",
    "    def build_feature_dataset(self, scada, event_info):\n",
    "        \"\"\"\n",
    "        Step 3: Create aggregated feature dataset\n",
    "        \"\"\"\n",
    "        print(\"\\nSTEP 3: FEATURE ENGINEERING\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        sensor_cols = [col for col in scada.columns \n",
    "                      if col not in ['time_stamp', 'asset_id', 'status_type_id']]\n",
    "        \n",
    "        print(f\"  Sensor columns: {len(sensor_cols)}\")\n",
    "        \n",
    "        # Build failure windows\n",
    "        failure_windows = self._extract_failure_windows(scada, event_info, sensor_cols)\n",
    "        \n",
    "        # Build normal baseline\n",
    "        normal_windows = self._extract_normal_windows(scada, event_info, sensor_cols)\n",
    "        \n",
    "        # Combine\n",
    "        full_dataset = pd.concat([failure_windows, normal_windows], ignore_index=True)\n",
    "        \n",
    "        print(f\"\\n  Dataset created:\")\n",
    "        print(f\"    Total windows: {len(full_dataset)}\")\n",
    "        print(f\"    Failure windows: {(full_dataset['label']==1).sum()}\")\n",
    "        print(f\"    Normal windows: {(full_dataset['label']==0).sum()}\")\n",
    "        print(f\"    Features: {len([c for c in full_dataset.columns if c.endswith(('_mean','_std','_trend'))])}\")\n",
    "        \n",
    "        return full_dataset\n",
    "    \n",
    "    def _extract_failure_windows(self, scada, event_info, sensor_cols):\n",
    "        \"\"\"Extract 24h windows before each failure\"\"\"\n",
    "        \n",
    "        sensor_cols = [col for col in scada.columns \n",
    "                       if col not in ['time_stamp', 'asset_id', 'status_type_id']]\n",
    "        \n",
    "        all_windows = []\n",
    "        \n",
    "        # Process each failure event\n",
    "        for event_id in event_info['event_id']:\n",
    "            failure = event_info[event_info['event_id'] == event_id].iloc[0]\n",
    "            asset_id = failure['asset_id']\n",
    "            failure_start = failure['event_start']\n",
    "            \n",
    "            # Find last production time\n",
    "            production_data = scada[\n",
    "                (scada['asset_id'] == asset_id) &\n",
    "                (scada['time_stamp'] < failure_start) &\n",
    "                (scada['status_type_id'] == 0) &\n",
    "                (scada[self.power_sensor] > self.power_threshold)\n",
    "            ].sort_values('time_stamp')\n",
    "            \n",
    "            if len(production_data) == 0:\n",
    "                print(f\"Warning: No production data for event {event_id}\")\n",
    "                continue\n",
    "            \n",
    "            last_production = production_data.iloc[-1]['time_stamp']\n",
    "            window_start = last_production - pd.Timedelta(hours=24)\n",
    "            \n",
    "            # Extract 24h window\n",
    "            window_data = scada[\n",
    "                (scada['asset_id'] == asset_id) &\n",
    "                (scada['time_stamp'] >= window_start) &\n",
    "                (scada['time_stamp'] <= last_production)\n",
    "            ]\n",
    "            \n",
    "            if len(window_data) < 100:  # Ensure we have enough data\n",
    "                print(f\"Warning: Insufficient data for event {event_id} ({len(window_data)} records)\")\n",
    "                continue\n",
    "            \n",
    "            # Aggregate features\n",
    "            features = aggregate_window_features(window_data, sensor_cols)\n",
    "            features['label'] = 1  # Failure\n",
    "            features['event_id'] = event_id\n",
    "            features['asset_id'] = asset_id\n",
    "            features['failure_type'] = failure['event_description']\n",
    "            \n",
    "            all_windows.append(features)\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        feature_df = pd.DataFrame(all_windows)\n",
    "        \n",
    "        print(f\"\\nCreated features for {len(feature_df)} failure windows\")\n",
    "        print(f\"Feature count: {len([c for c in feature_df.columns if c not in ['label', 'event_id', 'asset_id', 'failure_type']])}\")\n",
    "        \n",
    "        return feature_df\n",
    "        \n",
    "    \n",
    "    def _extract_normal_windows(self, scada, event_info, sensor_cols, windows_per_asset=20):\n",
    "        sensor_cols = [col for col in scada.columns \n",
    "                   if col not in ['time_stamp', 'asset_id', 'status_type_id']]\n",
    "    \n",
    "        all_normal_windows = []\n",
    "        \n",
    "        # Get all unique assets\n",
    "        assets = scada['asset_id'].unique()\n",
    "        \n",
    "        for asset_id in assets:\n",
    "            print(f\"\\nProcessing asset {asset_id}...\")\n",
    "            \n",
    "            # Get all failure timestamps for this asset\n",
    "            asset_failures = event_info[event_info['asset_id'] == asset_id]\n",
    "            \n",
    "            # Build exclusion zones (±30 days around each failure)\n",
    "            exclusion_zones = []\n",
    "            for _, failure in asset_failures.iterrows():\n",
    "                start_exclude = failure['event_start'] - pd.Timedelta(days=self.buffer_days)\n",
    "                end_exclude = failure['event_end'] + pd.Timedelta(days=self.buffer_days)\n",
    "                exclusion_zones.append((start_exclude, end_exclude))\n",
    "            \n",
    "            # Get asset data in normal operation\n",
    "            asset_data = scada[\n",
    "                (scada['asset_id'] == asset_id) &\n",
    "                (scada['status_type_id'] == 0) &\n",
    "                (scada[self.power_sensor] > self.power_threshold)\n",
    "            ].copy()\n",
    "            \n",
    "            # Filter out exclusion zones\n",
    "            mask = pd.Series(True, index=asset_data.index)\n",
    "            for start_ex, end_ex in exclusion_zones:\n",
    "                mask &= ~((asset_data['time_stamp'] >= start_ex) & \n",
    "                          (asset_data['time_stamp'] <= end_ex))\n",
    "            \n",
    "            normal_data = asset_data[mask].sort_values('time_stamp')\n",
    "            \n",
    "            if len(normal_data) < 144 * windows_per_asset:\n",
    "                print(f\"  Warning: Limited normal data for asset {asset_id}\")\n",
    "                continue\n",
    "            \n",
    "            # Sample random 24h windows\n",
    "            sampled_windows = 0\n",
    "            max_attempts = windows_per_asset * 3\n",
    "            attempts = 0\n",
    "            \n",
    "            while sampled_windows < windows_per_asset and attempts < max_attempts:\n",
    "                attempts += 1\n",
    "                \n",
    "                # Pick random starting point\n",
    "                max_start_idx = len(normal_data) - 145\n",
    "                if max_start_idx < 0:\n",
    "                    break\n",
    "                    \n",
    "                start_idx = np.random.randint(0, max_start_idx)\n",
    "                window_candidate = normal_data.iloc[start_idx:start_idx + 145]\n",
    "                \n",
    "                # Check if window is contiguous (no big gaps)\n",
    "                time_diffs = window_candidate['time_stamp'].diff()\n",
    "                if time_diffs.max() > pd.Timedelta(minutes=30):  # Allow small gaps\n",
    "                    continue\n",
    "                \n",
    "                # Aggregate features\n",
    "                features = aggregate_window_features(window_candidate, sensor_cols)\n",
    "                features['label'] = 0  # Normal\n",
    "                features['event_id'] = None\n",
    "                features['asset_id'] = asset_id\n",
    "                features['failure_type'] = 'normal'\n",
    "                \n",
    "                all_normal_windows.append(features)\n",
    "                sampled_windows += 1\n",
    "            \n",
    "            print(f\"  Created {sampled_windows} normal windows\")\n",
    "        \n",
    "        normal_df = pd.DataFrame(all_normal_windows)\n",
    "        print(f\"\\nTotal normal windows: {len(normal_df)}\")\n",
    "        \n",
    "        return normal_df\n",
    "    \n",
    "    def select_features(self, dataset, feature_descriptions=None, \n",
    "                   cohens_d_threshold=0.8, max_features=15, \n",
    "                   correlation_threshold=0.9):\n",
    "        \"\"\"\n",
    "        Step 4: Feature selection based on discriminative power\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        dataset : DataFrame\n",
    "            Full dataset with features and labels\n",
    "        feature_descriptions : DataFrame, optional\n",
    "            Feature description metadata (for interpretability)\n",
    "        cohens_d_threshold : float\n",
    "            Minimum effect size for initial filtering\n",
    "        max_features : int\n",
    "            Maximum features to select\n",
    "        correlation_threshold : float\n",
    "            Correlation threshold for redundancy removal\n",
    "        \"\"\"\n",
    "        print(\"\\nSTEP 4: FEATURE SELECTION\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Get feature columns (exclude metadata)\n",
    "        feature_cols = [c for c in dataset.columns \n",
    "                        if c.endswith(('_mean', '_std', '_trend'))]\n",
    "        \n",
    "        print(f\"Starting features: {len(feature_cols)}\")\n",
    "        \n",
    "        X = dataset[feature_cols]\n",
    "        y = dataset['label']\n",
    "        \n",
    "        # STEP 4A: Univariate discriminative analysis\n",
    "        from scipy import stats\n",
    "        import numpy as np\n",
    "        \n",
    "        feature_analysis = []\n",
    "        \n",
    "        for feature in feature_cols:\n",
    "            normal_vals = dataset[dataset['label'] == 0][feature]\n",
    "            failure_vals = dataset[dataset['label'] == 1][feature]\n",
    "            \n",
    "            # Skip if all NaN\n",
    "            if normal_vals.isna().all() or failure_vals.isna().all():\n",
    "                continue\n",
    "            \n",
    "            # T-test\n",
    "            t_stat, p_value = stats.ttest_ind(normal_vals.dropna(), failure_vals.dropna(), \n",
    "                                               equal_var=False, nan_policy='omit')\n",
    "            \n",
    "            # Cohen's d\n",
    "            mean_diff = failure_vals.mean() - normal_vals.mean()\n",
    "            pooled_std = np.sqrt((normal_vals.std()**2 + failure_vals.std()**2) / 2)\n",
    "            cohens_d = abs(mean_diff / pooled_std) if pooled_std > 0 else 0\n",
    "            \n",
    "            feature_analysis.append({\n",
    "                'feature': feature,\n",
    "                'cohens_d': cohens_d,\n",
    "                'p_value': p_value,\n",
    "                'mean_diff': mean_diff,\n",
    "                'normal_mean': normal_vals.mean(),\n",
    "                'failure_mean': failure_vals.mean()\n",
    "            })\n",
    "        \n",
    "        feature_analysis_df = pd.DataFrame(feature_analysis)\n",
    "        feature_analysis_df = feature_analysis_df.sort_values('cohens_d', ascending=False)\n",
    "        \n",
    "        print(f\"\\nFeatures with effect size calculated: {len(feature_analysis_df)}\")\n",
    "        print(f\"\\nEffect size distribution:\")\n",
    "        print(f\"  Cohen's d > 1.0 (large): {(feature_analysis_df['cohens_d'] > 1.0).sum()}\")\n",
    "        print(f\"  Cohen's d > 0.8 (large): {(feature_analysis_df['cohens_d'] > 0.8).sum()}\")\n",
    "        print(f\"  Cohen's d > 0.6 (medium): {(feature_analysis_df['cohens_d'] > 0.6).sum()}\")\n",
    "        \n",
    "        print(\"\\nTop 30 discriminative features:\")\n",
    "        print(feature_analysis_df.head(30)[['feature', 'cohens_d', 'mean_diff']].to_string(index=False))\n",
    "        \n",
    "        # STEP 4B: Iterative redundancy removal\n",
    "        print(\"\\nSTEP 4B: REDUNDANCY REMOVAL\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        selected_features = []\n",
    "        candidate_features = feature_analysis_df[\n",
    "            feature_analysis_df['cohens_d'] > cohens_d_threshold\n",
    "        ]['feature'].tolist()\n",
    "        \n",
    "        print(f\"Candidates (d > {cohens_d_threshold}): {len(candidate_features)}\")\n",
    "        \n",
    "        for candidate in candidate_features:\n",
    "            if len(selected_features) >= max_features:\n",
    "                break\n",
    "            \n",
    "            if len(selected_features) == 0:\n",
    "                # First feature - take highest Cohen's d\n",
    "                selected_features.append(candidate)\n",
    "                continue\n",
    "            \n",
    "            # Check correlation with already selected features\n",
    "            corr_check = dataset[selected_features + [candidate]].corr()\n",
    "            max_corr = corr_check[candidate][:-1].abs().max()\n",
    "            \n",
    "            if max_corr < correlation_threshold:\n",
    "                selected_features.append(candidate)\n",
    "        \n",
    "        print(f\"\\nSelected {len(selected_features)} non-redundant features\")\n",
    "        print(f\"(max_corr < {correlation_threshold}, Cohen's d > {cohens_d_threshold})\")\n",
    "        \n",
    "        print(\"\\n\\nFINAL FEATURE SET:\")\n",
    "        print(\"=\"*60)\n",
    "        for f in selected_features:\n",
    "            row = feature_analysis_df[feature_analysis_df['feature']==f].iloc[0]\n",
    "            print(f\"  {f:<40} d={row['cohens_d']:.3f}\")\n",
    "        \n",
    "        # Save to instance\n",
    "        self.selected_features = selected_features\n",
    "        self.feature_analysis = feature_analysis_df\n",
    "        \n",
    "        return selected_features\n",
    "        \n",
    "    def evaluate_model(self, full_dataset, selected_features, model_params=None):\n",
    "        \"\"\"\n",
    "        Step 5: Model training and Leave-One-Out cross-validation\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        dataset : DataFrame\n",
    "            Complete dataset with features and labels\n",
    "        selected_features : list\n",
    "            List of feature names to use\n",
    "        model_params : dict, optional\n",
    "            RandomForest parameters (default: sensible defaults)\n",
    "        \"\"\"\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        from sklearn.model_selection import LeaveOneOut\n",
    "        from sklearn.metrics import classification_report, confusion_matrix\n",
    "        import numpy as np\n",
    "        \n",
    "        print(\"\\n\\nSTEP 5: MODEL EVALUATION\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        if model_params is None:\n",
    "            model_params = {\n",
    "                'n_estimators': 100,\n",
    "                'max_depth': 5,\n",
    "                'random_state': 42,\n",
    "                'class_weight': 'balanced'\n",
    "            }\n",
    "        \n",
    "        X = dataset[selected_features]\n",
    "        y = dataset['label']\n",
    "        \n",
    "        print(f\"Features: {len(selected_features)}\")\n",
    "        print(f\"Samples: {len(X)} ({y.sum()} failures, {(~y.astype(bool)).sum()} normal)\")\n",
    "        \n",
    "        # Leave-One-Out Cross-Validation\n",
    "        loo = LeaveOneOut()\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        y_pred_proba = []\n",
    "        \n",
    "        for train_idx, test_idx in loo.split(X):\n",
    "            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "            \n",
    "            clf = RandomForestClassifier(**model_params)\n",
    "            clf.fit(X_train, y_train)\n",
    "            \n",
    "            y_true.append(y_test.values[0])\n",
    "            y_pred.append(clf.predict(X_test)[0])\n",
    "            y_pred_proba.append(clf.predict_proba(X_test)[0, 1])\n",
    "        \n",
    "        y_true = np.array(y_true)\n",
    "        y_pred = np.array(y_pred)\n",
    "        y_pred_proba = np.array(y_pred_proba)\n",
    "        \n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        print(cm)\n",
    "        \n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_true, y_pred, target_names=['Normal', 'Failure']))\n",
    "        \n",
    "        # Failure detection details\n",
    "        failure_indices = np.where(y_true == 1)[0]\n",
    "        detected = y_pred[failure_indices].sum()\n",
    "        total = len(failure_indices)\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"FAILURES DETECTED: {detected} / {total} ({detected/total*100:.1f}%)\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        print(\"\\nPer-failure probabilities:\")\n",
    "        print(f\"{'Event ID':<12} {'Predicted':<12} {'Probability':<12}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for idx in failure_indices:\n",
    "            event_id = dataset.iloc[idx]['event_id']\n",
    "            prob = y_pred_proba[idx]\n",
    "            pred = 'FAILURE ✓' if y_pred[idx] == 1 else 'normal ✗'\n",
    "            print(f\"{str(event_id):<12} {pred:<12} {prob:.3f}\")\n",
    "        \n",
    "        # Summary statistics\n",
    "        if detected > 0:\n",
    "            detected_probs = y_pred_proba[failure_indices][y_pred[failure_indices] == 1]\n",
    "            print(f\"\\nDetected failures - probability range: {detected_probs.min():.3f} to {detected_probs.max():.3f}\")\n",
    "        \n",
    "        if detected < total:\n",
    "            missed_probs = y_pred_proba[failure_indices][y_pred[failure_indices] == 0]\n",
    "            print(f\"Missed failures - probability range: {missed_probs.min():.3f} to {missed_probs.max():.3f}\")\n",
    "        \n",
    "        # Save results\n",
    "        self.model_results = {\n",
    "            'y_true': y_true,\n",
    "            'y_pred': y_pred,\n",
    "            'y_pred_proba': y_pred_proba,\n",
    "            'confusion_matrix': cm,\n",
    "            'recall': detected / total,\n",
    "            'precision': cm[1,1] / (cm[1,1] + cm[0,1]) if (cm[1,1] + cm[0,1]) > 0 else 0,\n",
    "            'accuracy': (cm[0,0] + cm[1,1]) / cm.sum()\n",
    "        }\n",
    "        \n",
    "        return self.model_results\n",
    "\n",
    "# Usage for Wind Farm B:\n",
    "# pipeline = WindTurbineFailurePipeline()\n",
    "# pipeline.validate_data_structure(scada_b, event_info_b)\n",
    "# gaps = pipeline.analyze_temporal_patterns(scada_b, event_info_b)\n",
    "# dataset = pipeline.build_feature_dataset(scada_b, event_info_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "436732bb-e20a-4622-a152-e9ad289ae9c2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def aggregate_window_features(window_data, sensor_cols):\n",
    "    \"\"\"\n",
    "    Aggregate a 24-hour window into summary statistics per sensor.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    window_data : DataFrame\n",
    "        24 hours of SCADA data (should be ~144 records at 10-min intervals)\n",
    "    sensor_cols : list\n",
    "        List of sensor column names to aggregate\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Feature dictionary for this window\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    for sensor in sensor_cols:\n",
    "        # Skip if sensor is all NaN in this window\n",
    "        if window_data[sensor].isna().all():\n",
    "            continue\n",
    "            \n",
    "        # Mean: average sensor value over 24h\n",
    "        features[f'{sensor}_mean'] = window_data[sensor].mean()\n",
    "        \n",
    "        # Std: variability over 24h\n",
    "        features[f'{sensor}_std'] = window_data[sensor].std()\n",
    "        \n",
    "        # Trend: linear slope over the window\n",
    "        values = window_data[sensor].dropna().values\n",
    "        if len(values) > 10:  # Need minimum points for meaningful trend\n",
    "            time_idx = np.arange(len(values))\n",
    "            slope, _ = np.polyfit(time_idx, values, 1)\n",
    "            features[f'{sensor}_trend'] = slope\n",
    "        else:\n",
    "            features[f'{sensor}_trend'] = 0\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72601b4-adab-46b3-b9b5-f5e94fcb2b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "farm = 'A'\n",
    "cohens_d_threshold = 1.0 # Higher for farms with many sensors\n",
    "max_features = 50\n",
    "correlation_threshold = 0.9\n",
    "\n",
    "match farm:\n",
    "    case 'A':\n",
    "        power_sensor = 'power_30_avg'\n",
    "    case 'B':\n",
    "        power_sensor = 'power_62_avg'\n",
    "    case 'C':\n",
    "        power_sensor = 'power_2_avg'\n",
    "\n",
    "scada = get_farm_scada_chunked(farm=farm)\n",
    "event_info = get_event_info(farm=farm)\n",
    "pipeline = WindTurbineFailurePipeline(power_sensor=power_sensor)\n",
    "pipeline.validate_data_structure(scada, event_info)\n",
    "gaps = pipeline.analyze_temporal_patterns(scada, event_info)\n",
    "dataset = pipeline.build_feature_dataset(scada, event_info)\n",
    "# Feature selection - now farm-independent\n",
    "selected_features = pipeline.select_features(\n",
    "    dataset=dataset,\n",
    "    cohens_d_threshold=cohens_d_threshold,  \n",
    "    max_features=max_features,\n",
    "    correlation_threshold=correlation_threshold\n",
    ")\n",
    "results = pipeline.evaluate_model(dataset, selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce92747e-f7ee-46ab-b3d7-cb3f36b13679",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
