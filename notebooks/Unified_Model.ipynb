{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cf675dc-e588-469a-8c4a-723de9fa433e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import LeaveOneOut, cross_val_predict\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "BASE_DIR = r\"D:\\Data\\SCADA\\Wind_Turbine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96f7ac58-bcdf-4bad-af16-d50fbd8d2b7d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_farm_scada_chunked(farm='A', asset_ids=None, chunksize=50000):\n",
    "    \"\"\"\n",
    "    Process farm data in chunks - for when dataset won't fit in RAM.\n",
    "    \n",
    "    asset_ids: Process only specific assets (e.g., [0, 10, 21])\n",
    "    chunksize: Rows per chunk\n",
    "    \"\"\"\n",
    "    farm_dir = os.path.join(BASE_DIR, 'Wind Farm '+farm)\n",
    "    farm_dataset_dir = os.path.join(farm_dir, 'datasets')\n",
    "    all_files = glob.glob(os.path.join(farm_dataset_dir, '*.csv'))\n",
    "    parquet_path = os.path.join(farm_dir, f'farm_{farm}_optimized.parquet')\n",
    "    dtype_dict = {\n",
    "        'asset_id': 'int16',\n",
    "        'status_type_id': 'int8',\n",
    "    }\n",
    "    # Check if already processed\n",
    "    if os.path.exists(parquet_path):\n",
    "        print(f\"Loading pre-processed Farm {farm}...\")\n",
    "        return pd.read_parquet(parquet_path)\n",
    "        \n",
    "    all_data = []\n",
    "    \n",
    "    for f in all_files:\n",
    "        # Read in chunks\n",
    "        for chunk in pd.read_csv(f, sep=\";\", dtype=dtype_dict, chunksize=chunksize):\n",
    "            # Filter to specific assets if provided\n",
    "            if asset_ids is not None:\n",
    "                chunk = chunk[chunk['asset_id'].isin(asset_ids)]\n",
    "            \n",
    "            # Optimize dtypes\n",
    "            float_cols = chunk.select_dtypes(include=['float64']).columns\n",
    "            chunk[float_cols] = chunk[float_cols].astype('float32')\n",
    "            \n",
    "            # Clean\n",
    "            chunk[\"time_stamp\"] = pd.to_datetime(chunk[\"time_stamp\"])\n",
    "            chunk = chunk.drop(['train_test', 'id'], axis=1, errors='ignore')\n",
    "            chunk = chunk.dropna(subset=[\"asset_id\", \"time_stamp\"])\n",
    "            \n",
    "            all_data.append(chunk)\n",
    "    \n",
    "    scada_data = pd.concat(all_data, ignore_index=True)\n",
    "    scada_data = scada_data.drop_duplicates(subset=['asset_id', 'time_stamp'], keep='first')\n",
    "    scada_data = scada_data.sort_values([\"asset_id\", \"time_stamp\"]).reset_index(drop=True)\n",
    "\n",
    "    scada_data.to_parquet(parquet_path, compression='snappy', index=False)\n",
    "    print(f\"Saved to {parquet_path}\")\n",
    "    \n",
    "    return scada_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01e88fc3-efd3-485c-a466-f2c96c52a678",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_event_info(farm='A'):\n",
    "    farm_dir = os.path.join(BASE_DIR, 'Wind Farm '+farm)\n",
    "    event_info = pd.read_csv(farm_dir + '\\\\event_info.csv', sep=';')\n",
    "    event_info = event_info.rename(columns={'asset':'asset_id'})\n",
    "    # Drop events that aren't anomalies\n",
    "    event_info = event_info[event_info['event_label'] == 'anomaly']\n",
    "    # Clean Up\n",
    "    event_info[\"event_start\"] = pd.to_datetime(event_info[\"event_start\"])\n",
    "    event_info[\"event_end\"] = pd.to_datetime(event_info[\"event_end\"])\n",
    "    event_info[\"asset_id\"] = pd.to_numeric(event_info[\"asset_id\"], errors=\"coerce\").astype(\"Int16\")\n",
    "    # Drop rows with missing critical info\n",
    "    event_info = event_info.dropna(subset=[\"asset_id\", \"event_start\", 'event_end'])\n",
    "    # Sort for asset_id\n",
    "    event_info = event_info.sort_values([\"asset_id\"]).reset_index(drop=True)\n",
    "\n",
    "    return event_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e318bd3c-5eb8-4ee5-8e07-276419847227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unified_dataset(scada_dict, event_info_dict, sensor_mapping, \n",
    "                         power_sensors, power_threshold=0.1, \n",
    "                         window_hours=24, buffer_days=30, random_seed=42):\n",
    "    \n",
    "    all_windows = []\n",
    "    \n",
    "    for farm in ['A', 'B', 'C']:\n",
    "        print(f\"\\nProcessing Farm {farm}...\")\n",
    "        \n",
    "        scada = scada_dict[farm]\n",
    "        events = event_info_dict[farm]\n",
    "        power_col = power_sensors[farm]\n",
    "        \n",
    "        failure_windows = extract_failure_windows_unified_fixed(\n",
    "            scada, events, farm, sensor_mapping, \n",
    "            power_col, power_threshold, window_hours\n",
    "        )\n",
    "        \n",
    "        print(f\"  Failure windows: {len(failure_windows)}\")\n",
    "        \n",
    "        normal_windows = extract_normal_windows_unified(\n",
    "            scada, events, farm, sensor_mapping,\n",
    "            power_col, power_threshold, window_hours, buffer_days,random_seed=random_seed\n",
    "        )\n",
    "        \n",
    "        print(f\"  Normal windows: {len(normal_windows)}\")\n",
    "        \n",
    "        # Combine\n",
    "        all_windows.extend(failure_windows)\n",
    "        all_windows.extend(normal_windows)\n",
    "    \n",
    "    dataset = pd.DataFrame(all_windows)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"UNIFIED DATASET CREATED\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total windows: {len(dataset)}\")\n",
    "    print(f\"Failure windows: {(dataset['label']==1).sum()}\")\n",
    "    print(f\"Normal windows: {(dataset['label']==0).sum()}\")\n",
    "    print(f\"Features: {len([c for c in dataset.columns if c not in ['label', 'event_id', 'asset_id', 'farm', 'failure_type']])}\")\n",
    "    print(f\"\\nWindows per farm:\")\n",
    "    print(dataset['farm'].value_counts())\n",
    "    \n",
    "    return dataset\n",
    "    print('Unified DB built')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e3fc821-e900-442a-bcdb-1ff4e7b2b86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_failure_windows_unified_fixed(scada, events, farm, sensor_mapping,\n",
    "                                         power_col, power_threshold, window_hours):    \n",
    "    windows = []\n",
    "    \n",
    "    for _, failure in events[events['event_label'] == 'anomaly'].iterrows():\n",
    "        \n",
    "        asset_id = failure['asset_id']\n",
    "        event_id = failure['event_id']\n",
    "        failure_start = failure['event_start']\n",
    "        \n",
    "        # Find last production time\n",
    "        production_data = scada[\n",
    "            (scada['asset_id'] == asset_id) &\n",
    "            (scada['time_stamp'] < failure_start) &\n",
    "            (scada['status_type_id'] == 0) &\n",
    "            (scada[power_col] > power_threshold)\n",
    "        ].sort_values('time_stamp')\n",
    "        \n",
    "        if len(production_data) == 0:\n",
    "            continue\n",
    "        \n",
    "        last_production = production_data.iloc[-1]['time_stamp']\n",
    "        window_start = last_production - pd.Timedelta(hours=window_hours)\n",
    "        \n",
    "        window_data = scada[\n",
    "            (scada['asset_id'] == asset_id) &\n",
    "            (scada['time_stamp'] >= window_start) &\n",
    "            (scada['time_stamp'] <= last_production)\n",
    "        ]\n",
    "        \n",
    "        if len(window_data) < 100:\n",
    "            continue\n",
    "        \n",
    "        features = extract_unified_window_features_fixed(\n",
    "            window_data, farm, sensor_mapping,\n",
    "            statistics=['mean', 'std', 'trend']\n",
    "        )\n",
    "        \n",
    "        features['label'] = 1\n",
    "        features['event_id'] = event_id\n",
    "        features['asset_id'] = asset_id\n",
    "        features['farm'] = farm\n",
    "        features['failure_type'] = failure.get('event_description', 'Unknown')\n",
    "        \n",
    "        windows.append(features)\n",
    "    \n",
    "    return windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bfb4e4eb-29a8-469b-91ed-8b03331941b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_normal_windows_unified(scada, events, farm, sensor_mapping,\n",
    "                                   power_col, power_threshold, window_hours,\n",
    "                                   buffer_days, windows_per_asset=20, random_seed=42):\n",
    "    np.random.seed(random_seed)\n",
    "    windows = []\n",
    "    \n",
    "    assets = scada['asset_id'].unique()\n",
    "    \n",
    "    for asset_id in assets:\n",
    "        \n",
    "        asset_failures = events[\n",
    "            (events['asset_id'] == asset_id) &\n",
    "            (events['event_label'] == 'anomaly')\n",
    "        ]   \n",
    "        exclusion_zones = []\n",
    "        for _, failure in asset_failures.iterrows():\n",
    "            start_exclude = failure['event_start'] - pd.Timedelta(days=buffer_days)\n",
    "            end_exclude = failure['event_end'] + pd.Timedelta(days=buffer_days)\n",
    "            exclusion_zones.append((start_exclude, end_exclude))\n",
    "        \n",
    "        asset_data = scada[\n",
    "            (scada['asset_id'] == asset_id) &\n",
    "            (scada['status_type_id'] == 0) &\n",
    "            (scada[power_col] > power_threshold)\n",
    "        ].copy()\n",
    "        \n",
    "        mask = pd.Series(True, index=asset_data.index)\n",
    "        for start_ex, end_ex in exclusion_zones:\n",
    "            mask &= ~((asset_data['time_stamp'] >= start_ex) & \n",
    "                      (asset_data['time_stamp'] <= end_ex))\n",
    "        \n",
    "        normal_data = asset_data[mask].sort_values('time_stamp')\n",
    "        \n",
    "        if len(normal_data) < 144 * windows_per_asset:\n",
    "            continue\n",
    "        \n",
    "        sampled = 0\n",
    "        max_attempts = windows_per_asset * 3\n",
    "        attempts = 0\n",
    "        \n",
    "        while sampled < windows_per_asset and attempts < max_attempts:\n",
    "            attempts += 1\n",
    "            \n",
    "            max_start_idx = len(normal_data) - 145\n",
    "            if max_start_idx < 0:\n",
    "                break\n",
    "            \n",
    "            start_idx = np.random.randint(0, max_start_idx)\n",
    "            window_candidate = normal_data.iloc[start_idx:start_idx + 145]\n",
    "            \n",
    "            time_diffs = window_candidate['time_stamp'].diff()\n",
    "            if time_diffs.max() > pd.Timedelta(minutes=30):\n",
    "                continue\n",
    "            \n",
    "            features = extract_unified_window_features_fixed(\n",
    "                window_data=window_candidate,  \n",
    "                farm=farm,\n",
    "                sensor_mapping=sensor_mapping,\n",
    "                statistics=['mean', 'std', 'trend']\n",
    "            )\n",
    "            \n",
    "            features['label'] = 0\n",
    "            features['event_id'] = None\n",
    "            features['asset_id'] = asset_id\n",
    "            features['farm'] = farm\n",
    "            features['failure_type'] = 'normal'\n",
    "            \n",
    "            windows.append(features)\n",
    "            sampled += 1\n",
    "    \n",
    "    return windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3522d5c5-cd0f-4ab7-b745-845e5d467cf4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def extract_unified_window_features_fixed(window_data, farm, sensor_mapping, \n",
    "                                    statistics=['mean', 'std', 'trend', 'max', 'min']):\n",
    "    \n",
    "    farm_sensors = sensor_mapping[sensor_mapping['farm'] == farm].copy()\n",
    "    \n",
    "    unified_features = {}\n",
    "    \n",
    "    for (primary, secondary), group in farm_sensors.groupby(['primary group', 'secondary group']):\n",
    "        \n",
    "        category_key = f\"{primary}_{secondary}\"\n",
    "        \n",
    "        mapping_sensor_names = group['sensor_name'].tolist()\n",
    "        \n",
    "        available_sensors = []\n",
    "        for col in window_data.columns:\n",
    "            root_name = extract_root_sensor_name(col)\n",
    "            if root_name in mapping_sensor_names:\n",
    "                available_sensors.append(col)\n",
    "        \n",
    "        if len(available_sensors) == 0:\n",
    "            continue\n",
    "        \n",
    "        category_data = window_data[available_sensors]\n",
    "        \n",
    "        if len(available_sensors) > 1:\n",
    "            category_series = category_data.mean(axis=1, skipna=True)\n",
    "        else:\n",
    "            category_series = category_data[available_sensors[0]]\n",
    "        \n",
    "        if category_series.isna().all():\n",
    "            continue\n",
    "        \n",
    "        if 'mean' in statistics:\n",
    "            unified_features[f\"{category_key}_mean\"] = category_series.mean()\n",
    "        \n",
    "        if 'std' in statistics:\n",
    "            unified_features[f\"{category_key}_std\"] = category_series.std()\n",
    "        \n",
    "        if 'trend' in statistics:\n",
    "            unified_features[f\"{category_key}_trend\"] = compute_linear_trend(category_series)\n",
    "        \n",
    "        if 'max' in statistics:\n",
    "            unified_features[f\"{category_key}_max\"] = category_series.max()\n",
    "        \n",
    "        if 'min' in statistics:\n",
    "            unified_features[f\"{category_key}_min\"] = category_series.min()\n",
    "    \n",
    "    return unified_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df532d3f-79b4-4bab-a1bd-9060789af8a4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def compute_linear_trend(series):\n",
    "    \n",
    "    series_clean = series.dropna()\n",
    "    if len(series_clean) < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    x = np.arange(len(series_clean))\n",
    "    try:\n",
    "        slope, _ = np.polyfit(x, series_clean, 1)\n",
    "        return slope\n",
    "    except:\n",
    "        return 0.0\n",
    "    print('linear trend done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd456095-ee9d-4aa6-a855-f38c828dc235",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def analyze_unified_features(dataset):\n",
    "    \n",
    "    feature_cols = [c for c in dataset.columns \n",
    "                   if c not in ['label', 'event_id', 'asset_id', 'farm', 'failure_type']]\n",
    "    \n",
    "    print(f\"\\nTotal unified features: {len(feature_cols)}\")\n",
    "    \n",
    "    if len(feature_cols) == 0:\n",
    "        print(\"ERROR: No feature columns found!\")\n",
    "        print(\"Available columns:\", dataset.columns.tolist())\n",
    "        return None\n",
    "    \n",
    "    stat_types = {}\n",
    "    for stat in ['mean', 'std', 'trend', 'max', 'min']:\n",
    "        count = len([c for c in feature_cols if c.endswith(f'_{stat}')])\n",
    "        stat_types[stat] = count\n",
    "    \n",
    "    print(\"\\nFeatures by statistic type:\")\n",
    "    for stat, count in stat_types.items():\n",
    "        print(f\"  {stat}: {count}\")\n",
    "    \n",
    "    # Check coverage across farms\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Feature Coverage by Farm:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for farm in ['A', 'B', 'C']:\n",
    "        farm_data = dataset[dataset['farm'] == farm]\n",
    "        if len(farm_data) == 0:\n",
    "            print(f\"\\nFarm {farm}: NO DATA\")\n",
    "            continue\n",
    "            \n",
    "        non_null = farm_data[feature_cols].notna().sum()\n",
    "        coverage = (non_null / len(farm_data) * 100).describe()\n",
    "        \n",
    "        print(f\"\\nFarm {farm} ({len(farm_data)} windows):\")\n",
    "        print(f\"  Mean coverage: {coverage['mean']:.1f}%\")\n",
    "        print(f\"  Min coverage: {coverage['min']:.1f}%\")\n",
    "        print(f\"  Features with 100% coverage: {(non_null == len(farm_data)).sum()}\")\n",
    "    \n",
    "    # Missing data analysis\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Missing Data Analysis:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    missing_pct = (dataset[feature_cols].isna().sum() / len(dataset) * 100).sort_values(ascending=False)\n",
    "    \n",
    "    print(f\"\\nFeatures with >50% missing: {(missing_pct > 50).sum()}\")\n",
    "    print(f\"Features with >25% missing: {(missing_pct > 25).sum()}\")\n",
    "    print(f\"Features with >10% missing: {(missing_pct > 10).sum()}\")\n",
    "    print(f\"Features with 0% missing: {(missing_pct == 0).sum()}\")\n",
    "    \n",
    "    if (missing_pct > 50).sum() > 0:\n",
    "        print(\"\\nTop 20 features with most missing data:\")\n",
    "        print(missing_pct.head(20))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Discriminative Power Analysis:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    discriminative_features = []\n",
    "    skipped_missing = 0\n",
    "    skipped_error = 0\n",
    "    \n",
    "    for feature in feature_cols:\n",
    "        normal_vals = dataset[dataset['label'] == 0][feature]\n",
    "        failure_vals = dataset[dataset['label'] == 1][feature]\n",
    "        \n",
    "        # RELAXED: Skip only if >75% missing (was 50%)\n",
    "        normal_missing_pct = normal_vals.isna().sum() / len(normal_vals)\n",
    "        failure_missing_pct = failure_vals.isna().sum() / len(failure_vals)\n",
    "        \n",
    "        if normal_missing_pct > 0.75 or failure_missing_pct > 0.75:\n",
    "            skipped_missing += 1\n",
    "            continue\n",
    "        \n",
    "        normal_clean = normal_vals.dropna()\n",
    "        failure_clean = failure_vals.dropna()\n",
    "        \n",
    "        if len(normal_clean) < 3 or len(failure_clean) < 3:\n",
    "            skipped_error += 1\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            mean_diff = failure_clean.mean() - normal_clean.mean()\n",
    "            pooled_std = np.sqrt((normal_clean.std()**2 + failure_clean.std()**2) / 2)\n",
    "            cohens_d = abs(mean_diff / pooled_std) if pooled_std > 0 else 0\n",
    "            \n",
    "            discriminative_features.append({\n",
    "                'feature': feature,\n",
    "                'cohens_d': cohens_d,\n",
    "                'mean_diff': mean_diff,\n",
    "                'normal_missing_pct': normal_missing_pct,\n",
    "                'failure_missing_pct': failure_missing_pct\n",
    "            })\n",
    "        except Exception as e:\n",
    "            skipped_error += 1\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nFeatures analyzed: {len(discriminative_features)}\")\n",
    "    print(f\"Skipped (>75% missing): {skipped_missing}\")\n",
    "    print(f\"Skipped (errors/insufficient data): {skipped_error}\")\n",
    "    \n",
    "    if len(discriminative_features) == 0:\n",
    "        print(\"\\nERROR: No features could be analyzed!\")\n",
    "        print(\"This suggests all features have too much missing data.\")\n",
    "        print(\"\\nDEBUG: Check first few feature columns:\")\n",
    "        for col in feature_cols[:5]:\n",
    "            print(f\"  {col}: {dataset[col].isna().sum()}/{len(dataset)} missing\")\n",
    "        return None\n",
    "    \n",
    "    df_disc = pd.DataFrame(discriminative_features).sort_values('cohens_d', ascending=False)\n",
    "    \n",
    "    print(f\"Features with d > 1.0: {(df_disc['cohens_d'] > 1.0).sum()}\")\n",
    "    print(f\"Features with d > 0.8: {(df_disc['cohens_d'] > 0.8).sum()}\")\n",
    "    print(f\"Features with d > 0.6: {(df_disc['cohens_d'] > 0.6).sum()}\")\n",
    "    print(f\"Features with d > 0.5: {(df_disc['cohens_d'] > 0.5).sum()}\")\n",
    "    \n",
    "    print(\"\\nTop 20 discriminative unified features:\")\n",
    "    print(df_disc.head(20)[['feature', 'cohens_d', 'mean_diff']].to_string(index=False))\n",
    "    \n",
    "    return df_disc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f735dc1d-84a5-4541-bcfc-c3be510406fe",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def select_unified_features(dataset, cohens_d_threshold=0.6, max_features=20, \n",
    "                           correlation_threshold=0.9):\n",
    "    \n",
    "    feature_cols = [c for c in dataset.columns \n",
    "                   if c not in ['label', 'event_id', 'asset_id', 'farm', 'failure_type']]\n",
    "    \n",
    "    print(f\"\\nTotal feature columns: {len(feature_cols)}\")\n",
    "    \n",
    "    feature_analysis = []\n",
    "    skipped_missing = 0\n",
    "    skipped_error = 0\n",
    "    \n",
    "    for feature in feature_cols:\n",
    "        normal_vals = dataset[dataset['label'] == 0][feature]\n",
    "        failure_vals = dataset[dataset['label'] == 1][feature]\n",
    "        \n",
    "        # RELAXED: Skip only if >75% missing (was 50%)\n",
    "        normal_missing_pct = normal_vals.isna().sum() / len(normal_vals)\n",
    "        failure_missing_pct = failure_vals.isna().sum() / len(failure_vals)\n",
    "        \n",
    "        if normal_missing_pct > 0.75 or failure_missing_pct > 0.75:\n",
    "            skipped_missing += 1\n",
    "            continue\n",
    "        \n",
    "        normal_clean = normal_vals.dropna()\n",
    "        failure_clean = failure_vals.dropna()\n",
    "        \n",
    "        if len(normal_clean) < 3 or len(failure_clean) < 3:\n",
    "            skipped_error += 1\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            mean_diff = failure_clean.mean() - normal_clean.mean()\n",
    "            pooled_std = np.sqrt((normal_clean.std()**2 + failure_clean.std()**2) / 2)\n",
    "            cohens_d = abs(mean_diff / pooled_std) if pooled_std > 0 else 0\n",
    "            \n",
    "            feature_analysis.append({\n",
    "                'feature': feature,\n",
    "                'cohens_d': cohens_d\n",
    "            })\n",
    "        except Exception as e:\n",
    "            skipped_error += 1\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nFeatures analyzed: {len(feature_analysis)}\")\n",
    "    print(f\"Skipped (>75% missing): {skipped_missing}\")\n",
    "    print(f\"Skipped (errors/insufficient data): {skipped_error}\")\n",
    "    \n",
    "    if len(feature_analysis) == 0:\n",
    "        print(\"\\nERROR: No features could be analyzed!\")\n",
    "        return []\n",
    "    \n",
    "    df_analysis = pd.DataFrame(feature_analysis).sort_values('cohens_d', ascending=False)\n",
    "    \n",
    "    selected_features = []\n",
    "    candidates = df_analysis[df_analysis['cohens_d'] > cohens_d_threshold]['feature'].tolist()\n",
    "    \n",
    "    print(f\"\\nCandidates (d > {cohens_d_threshold}): {len(candidates)}\")\n",
    "    \n",
    "    if len(candidates) == 0:\n",
    "        print(f\"\\nWARNING: No features exceed Cohen's d threshold of {cohens_d_threshold}\")\n",
    "        print(\"Lowering threshold to include top 10 features...\")\n",
    "        candidates = df_analysis.head(10)['feature'].tolist()\n",
    "    \n",
    "    for candidate in candidates:\n",
    "        if len(selected_features) >= max_features:\n",
    "            break\n",
    "        \n",
    "        if len(selected_features) == 0:\n",
    "            selected_features.append(candidate)\n",
    "            continue\n",
    "        \n",
    "        # Check correlation with imputation for missing values\n",
    "        try:\n",
    "            subset = dataset[selected_features + [candidate]].copy()\n",
    "            \n",
    "            # Impute missing values\n",
    "            from sklearn.impute import SimpleImputer\n",
    "            imputer = SimpleImputer(strategy='median')\n",
    "            subset_imputed = pd.DataFrame(\n",
    "                imputer.fit_transform(subset),\n",
    "                columns=subset.columns\n",
    "            )\n",
    "            \n",
    "            corr_check = subset_imputed.corr()\n",
    "            max_corr = corr_check[candidate][:-1].abs().max()\n",
    "            \n",
    "            if max_corr < correlation_threshold:\n",
    "                selected_features.append(candidate)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nSelected {len(selected_features)} features\")\n",
    "    print(f\"(max_corr < {correlation_threshold}, d > {cohens_d_threshold})\")\n",
    "    \n",
    "    if len(selected_features) > 0:\n",
    "        print(\"\\n\\nFINAL UNIFIED FEATURE SET:\")\n",
    "        print(\"=\"*60)\n",
    "        for f in selected_features:\n",
    "            d = df_analysis[df_analysis['feature']==f]['cohens_d'].iloc[0]\n",
    "            print(f\"  {f:<50} d={d:.3f}\")\n",
    "    else:\n",
    "        print(\"\\nERROR: No features selected!\")\n",
    "    \n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e28c1769-ced5-4684-a534-a73e825eb478",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_unified_model(dataset, selected_features):\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"UNIFIED MODEL EVALUATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    X = dataset[selected_features]\n",
    "    y = dataset['label']\n",
    "    \n",
    "    print(f\"\\nFeatures: {len(selected_features)}\")\n",
    "    print(f\"Total samples: {len(X)}\")\n",
    "    print(f\"Failures: {y.sum()}\")\n",
    "    print(f\"Normal: {(y==0).sum()}\")\n",
    "    \n",
    "    from sklearn.impute import SimpleImputer\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    X_imputed = pd.DataFrame(\n",
    "        imputer.fit_transform(X),\n",
    "        columns=X.columns,\n",
    "        index=X.index\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nMissing values before imputation: {X.isna().sum().sum()}\")\n",
    "    print(f\"Missing values after imputation: {X_imputed.isna().sum().sum()}\")\n",
    "    \n",
    "    loo = LeaveOneOut()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_pred_proba = []\n",
    "    \n",
    "    print(\"\\nRunning Leave-One-Out Cross-Validation...\")\n",
    "    \n",
    "    for train_idx, test_idx in loo.split(X_imputed):\n",
    "        X_train, X_test = X_imputed.iloc[train_idx], X_imputed.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        \n",
    "        clf = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=5,\n",
    "            random_state=42,\n",
    "            class_weight='balanced'\n",
    "        )\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        y_true.append(y_test.values[0])\n",
    "        y_pred.append(clf.predict(X_test)[0])\n",
    "        y_pred_proba.append(clf.predict_proba(X_test)[0, 1])\n",
    "    \n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_pred_proba = np.array(y_pred_proba)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"OVERALL PERFORMANCE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=['Normal', 'Failure']))\n",
    "    \n",
    "    failure_indices = np.where(y_true == 1)[0]\n",
    "    detected = y_pred[failure_indices].sum()\n",
    "    total = len(failure_indices)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FAILURES DETECTED: {detected} / {total} ({detected/total*100:.1f}%)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PERFORMANCE BY FARM\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for farm in ['A', 'B', 'C']:\n",
    "        farm_mask = dataset['farm'] == farm\n",
    "        farm_indices = np.where(farm_mask)[0]\n",
    "        \n",
    "        y_true_farm = y_true[farm_indices]\n",
    "        y_pred_farm = y_pred[farm_indices]\n",
    "        \n",
    "        failure_mask = y_true_farm == 1\n",
    "        if failure_mask.sum() == 0:\n",
    "            continue\n",
    "        \n",
    "        farm_detected = y_pred_farm[failure_mask].sum()\n",
    "        farm_total = failure_mask.sum()\n",
    "        farm_recall = farm_detected / farm_total if farm_total > 0 else 0\n",
    "        \n",
    "        normal_mask = y_true_farm == 0\n",
    "        farm_fp = (y_pred_farm[normal_mask] == 1).sum()\n",
    "        farm_tn = (y_pred_farm[normal_mask] == 0).sum()\n",
    "        farm_precision = farm_detected / (farm_detected + farm_fp) if (farm_detected + farm_fp) > 0 else 0\n",
    "        \n",
    "        print(f\"\\nFarm {farm}:\")\n",
    "        print(f\"  Total failures: {farm_total}\")\n",
    "        print(f\"  Detected: {farm_detected} ({farm_recall:.0%})\")\n",
    "        print(f\"  False alarms: {farm_fp}/{farm_tn + farm_fp}\")\n",
    "        print(f\"  Precision: {farm_precision:.0%}\")\n",
    "    \n",
    "    return {\n",
    "        'y_true': y_true,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba,\n",
    "        'confusion_matrix': cm\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3994f192-dfd8-458f-b485-8e557c097f80",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def extract_root_sensor_name(col_name):\n",
    "    \n",
    "    suffixes = ['_min', '_max', '_avg', '_std', '_std_dev']\n",
    "    \n",
    "    for suffix in suffixes:\n",
    "        if col_name.endswith(suffix):\n",
    "            return col_name[:-len(suffix)]\n",
    "    \n",
    "    # If no suffix found, return as-is\n",
    "    return col_name\n",
    "\n",
    "\n",
    "def extract_unified_window_features_fixed(window_data, farm, sensor_mapping, \n",
    "                                          statistics=['mean', 'std', 'trend']):\n",
    "\n",
    "    farm_sensors = sensor_mapping[sensor_mapping['farm'] == farm].copy()\n",
    "    \n",
    "    unified_features = {}\n",
    "    \n",
    "    for (primary, secondary), group in farm_sensors.groupby(['primary group', 'secondary group']):\n",
    "        \n",
    "        category_key = f\"{primary}_{secondary}\"\n",
    "        \n",
    "        mapping_sensor_names = group['sensor_name'].tolist()\n",
    "        \n",
    "        available_sensors = []\n",
    "        for col in window_data.columns:\n",
    "            root_name = extract_root_sensor_name(col)\n",
    "            if root_name in mapping_sensor_names:\n",
    "                available_sensors.append(col)\n",
    "        \n",
    "        if len(available_sensors) == 0:\n",
    "            continue\n",
    "        \n",
    "        category_data = window_data[available_sensors]\n",
    "        \n",
    "        if len(available_sensors) > 1:\n",
    "            category_series = category_data.mean(axis=1, skipna=True)\n",
    "        else:\n",
    "            category_series = category_data[available_sensors[0]]\n",
    "        \n",
    "        if category_series.isna().all():\n",
    "            continue\n",
    "        \n",
    "        if 'mean' in statistics:\n",
    "            unified_features[f\"{category_key}_mean\"] = category_series.mean()\n",
    "        \n",
    "        if 'std' in statistics:\n",
    "            unified_features[f\"{category_key}_std\"] = category_series.std()\n",
    "        \n",
    "        if 'trend' in statistics:\n",
    "            unified_features[f\"{category_key}_trend\"] = compute_linear_trend(category_series)\n",
    "    \n",
    "    return unified_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bdb2c15e-8074-435b-8dc2-0428fac2e28e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-processed Farm A...\n",
      "Loading pre-processed Farm B...\n",
      "Loading pre-processed Farm C...\n",
      "\n",
      "Rebuilding unified dataset with merged categories...\n",
      "================================================================================\n",
      "\n",
      "Processing Farm A...\n",
      "  Failure windows: 12\n",
      "  Normal windows: 65\n",
      "\n",
      "Processing Farm B...\n",
      "  Failure windows: 6\n",
      "  Normal windows: 180\n",
      "\n",
      "Processing Farm C...\n",
      "  Failure windows: 27\n",
      "  Normal windows: 424\n",
      "\n",
      "============================================================\n",
      "UNIFIED DATASET CREATED\n",
      "============================================================\n",
      "Total windows: 714\n",
      "Failure windows: 45\n",
      "Normal windows: 669\n",
      "Features: 78\n",
      "\n",
      "Windows per farm:\n",
      "farm\n",
      "C    451\n",
      "B    186\n",
      "A     77\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Dataset shape: (714, 83)\n",
      "Total windows: 714\n",
      "Failure windows: 45\n",
      "Normal windows: 669\n",
      "\n",
      "Features: 78\n",
      "Missing data: 23.9% average\n",
      "Features with <25% missing: 42\n",
      "Features with <50% missing: 66\n",
      "\n",
      "Total unified features: 78\n",
      "\n",
      "Features by statistic type:\n",
      "  mean: 26\n",
      "  std: 26\n",
      "  trend: 26\n",
      "  max: 0\n",
      "  min: 0\n",
      "\n",
      "============================================================\n",
      "Feature Coverage by Farm:\n",
      "============================================================\n",
      "\n",
      "Farm A (77 windows):\n",
      "  Mean coverage: 61.5%\n",
      "  Min coverage: 0.0%\n",
      "  Features with 100% coverage: 48\n",
      "\n",
      "Farm B (186 windows):\n",
      "  Mean coverage: 61.5%\n",
      "  Min coverage: 0.0%\n",
      "  Features with 100% coverage: 48\n",
      "\n",
      "Farm C (451 windows):\n",
      "  Mean coverage: 84.6%\n",
      "  Min coverage: 0.0%\n",
      "  Features with 100% coverage: 66\n",
      "\n",
      "============================================================\n",
      "Missing Data Analysis:\n",
      "============================================================\n",
      "\n",
      "Features with >50% missing: 12\n",
      "Features with >25% missing: 36\n",
      "Features with >10% missing: 42\n",
      "Features with 0% missing: 36\n",
      "\n",
      "Top 20 features with most missing data:\n",
      "nacelle_controller_trend      89.215686\n",
      "nacelle_controller_std        89.215686\n",
      "nacelle_controller_mean       89.215686\n",
      "hydraulic_system_trend        89.215686\n",
      "hydraulic_system_std          89.215686\n",
      "hydraulic_system_mean         89.215686\n",
      "structural_vibration_mean     73.949580\n",
      "structural_vibration_std      73.949580\n",
      "structural_vibration_trend    73.949580\n",
      "tower_structural_mean         73.949580\n",
      "tower_structural_std          73.949580\n",
      "tower_structural_trend        73.949580\n",
      "hydraulic_aggregate_trend     36.834734\n",
      "hydraulic_aggregate_mean      36.834734\n",
      "hydraulic_aggregate_std       36.834734\n",
      "nacelle_pressure_trend        36.834734\n",
      "hydraulic_locking_std         36.834734\n",
      "hydraulic_locking_trend       36.834734\n",
      "hydraulic_oil_mean            36.834734\n",
      "hydraulic_oil_std             36.834734\n",
      "dtype: float64\n",
      "\n",
      "============================================================\n",
      "Discriminative Power Analysis:\n",
      "============================================================\n",
      "\n",
      "Features analyzed: 66\n",
      "Skipped (>75% missing): 12\n",
      "Skipped (errors/insufficient data): 0\n",
      "Features with d > 1.0: 1\n",
      "Features with d > 0.8: 2\n",
      "Features with d > 0.6: 7\n",
      "Features with d > 0.5: 11\n",
      "\n",
      "Top 20 discriminative unified features:\n",
      "                  feature  cohens_d    mean_diff\n",
      "          pitch_angle_std  1.228476     9.626135\n",
      " generator_electrical_std  0.858495    68.373810\n",
      "     rotor_mechanical_std  0.759435     1.226650\n",
      "        pitch_control_std  0.715616     3.154956\n",
      "         pitch_angle_mean  0.687875     7.492994\n",
      "  control_temperature_std  0.679632     0.648994\n",
      "    generator_thermal_std  0.616091     2.435657\n",
      "       power_reactive_std  0.565033   156.027435\n",
      "  gearbox_mechanical_mean  0.537155   -30.486980\n",
      "        rotor_thermal_std  0.523528     1.039160\n",
      "        pitch_angle_trend  0.523011    -0.094788\n",
      "hydraulic_aggregate_trend  0.499354     0.050205\n",
      "   gearbox_mechanical_std  0.469636     4.507868\n",
      "  gearbox_lubrication_std  0.436589     3.869029\n",
      "     electrical_grid_mean  0.422101 -1194.013184\n",
      "      electrical_grid_std  0.421475    56.332741\n",
      "    hydraulic_pumps_trend  0.420586     0.001251\n",
      "     environment_wind_std  0.415370     1.399646\n",
      "  hydraulic_locking_trend  0.392155     0.076755\n",
      "         power_active_std  0.386159  1705.141724\n",
      "\n",
      "Total feature columns: 78\n",
      "\n",
      "Features analyzed: 66\n",
      "Skipped (>75% missing): 12\n",
      "Skipped (errors/insufficient data): 0\n",
      "\n",
      "Candidates (d > 0.5): 11\n",
      "\n",
      "Selected 11 features\n",
      "(max_corr < 0.9, d > 0.5)\n",
      "\n",
      "\n",
      "FINAL UNIFIED FEATURE SET:\n",
      "============================================================\n",
      "  pitch_angle_std                                    d=1.228\n",
      "  generator_electrical_std                           d=0.858\n",
      "  rotor_mechanical_std                               d=0.759\n",
      "  pitch_control_std                                  d=0.716\n",
      "  pitch_angle_mean                                   d=0.688\n",
      "  control_temperature_std                            d=0.680\n",
      "  generator_thermal_std                              d=0.616\n",
      "  power_reactive_std                                 d=0.565\n",
      "  gearbox_mechanical_mean                            d=0.537\n",
      "  rotor_thermal_std                                  d=0.524\n",
      "  pitch_angle_trend                                  d=0.523\n",
      "\n",
      "================================================================================\n",
      "UNIFIED MODEL EVALUATION\n",
      "================================================================================\n",
      "\n",
      "Features: 11\n",
      "Total samples: 714\n",
      "Failures: 45\n",
      "Normal: 669\n",
      "\n",
      "Missing values before imputation: 263\n",
      "Missing values after imputation: 0\n",
      "\n",
      "Running Leave-One-Out Cross-Validation...\n",
      "\n",
      "============================================================\n",
      "OVERALL PERFORMANCE\n",
      "============================================================\n",
      "\n",
      "Confusion Matrix:\n",
      "[[668   1]\n",
      " [ 20  25]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.97      1.00      0.98       669\n",
      "     Failure       0.96      0.56      0.70        45\n",
      "\n",
      "    accuracy                           0.97       714\n",
      "   macro avg       0.97      0.78      0.84       714\n",
      "weighted avg       0.97      0.97      0.97       714\n",
      "\n",
      "\n",
      "============================================================\n",
      "FAILURES DETECTED: 25 / 45 (55.6%)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "PERFORMANCE BY FARM\n",
      "============================================================\n",
      "\n",
      "Farm A:\n",
      "  Total failures: 12\n",
      "  Detected: 7 (58%)\n",
      "  False alarms: 0/65\n",
      "  Precision: 100%\n",
      "\n",
      "Farm B:\n",
      "  Total failures: 6\n",
      "  Detected: 3 (50%)\n",
      "  False alarms: 1/180\n",
      "  Precision: 75%\n",
      "\n",
      "Farm C:\n",
      "  Total failures: 27\n",
      "  Detected: 15 (56%)\n",
      "  False alarms: 0/424\n",
      "  Precision: 100%\n"
     ]
    }
   ],
   "source": [
    "sensor_mapping_v2 = pd.read_csv(BASE_DIR+'\\\\sensor_mapping_v2.csv')\n",
    "\n",
    "scada_dict = {\n",
    "    'A': get_farm_scada_chunked(farm='A'),\n",
    "    'B': get_farm_scada_chunked(farm='B'),\n",
    "    'C': get_farm_scada_chunked(farm='C')\n",
    "}\n",
    "\n",
    "event_info_dict = {\n",
    "    'A': get_event_info(farm='A'),\n",
    "    'B': get_event_info(farm='B'),\n",
    "    'C': get_event_info(farm='C')\n",
    "}\n",
    "\n",
    "power_sensors = {\n",
    "    'A': 'power_30_avg',\n",
    "    'B': 'power_62_avg', \n",
    "    'C': 'power_2_avg'\n",
    "}\n",
    "print(\"\\nRebuilding unified dataset with merged categories...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Rebuild dataset using the merged sensor mapping\n",
    "dataset_unified_seeded = build_unified_dataset(\n",
    "    scada_dict=scada_dict,\n",
    "    event_info_dict=event_info_dict,\n",
    "    sensor_mapping=sensor_mapping_v2,\n",
    "    power_sensors=power_sensors,\n",
    "    power_threshold=0.1,\n",
    "    window_hours=24,\n",
    "    buffer_days=30,\n",
    "    random_seed=42  # Add this parameter!\n",
    ")\n",
    "\n",
    "# Quick diagnostic\n",
    "print(f\"\\nDataset shape: {dataset_unified_seeded.shape}\")\n",
    "print(f\"Total windows: {len(dataset_unified_seeded)}\")\n",
    "print(f\"Failure windows: {(dataset_unified_seeded['label']==1).sum()}\")\n",
    "print(f\"Normal windows: {(dataset_unified_seeded['label']==0).sum()}\")\n",
    "\n",
    "# Check missing data\n",
    "feature_cols = [c for c in dataset_unified_seeded.columns \n",
    "               if c not in ['label', 'event_id', 'asset_id', 'farm', 'failure_type']]\n",
    "missing_pct = (dataset_unified_seeded[feature_cols].isna().sum() / len(dataset_unified_seeded) * 100)\n",
    "\n",
    "print(f\"\\nFeatures: {len(feature_cols)}\")\n",
    "print(f\"Missing data: {missing_pct.mean():.1f}% average\")\n",
    "print(f\"Features with <25% missing: {(missing_pct < 25).sum()}\")\n",
    "print(f\"Features with <50% missing: {(missing_pct < 50).sum()}\")\n",
    "\n",
    "# Analyze the unified features\n",
    "df_discriminative_v2 = analyze_unified_features(dataset_unified_seeded)\n",
    "\n",
    "# Select features\n",
    "selected_unified_features = select_unified_features(\n",
    "    dataset=dataset_unified_seeded,\n",
    "    cohens_d_threshold=0.5,  # Relaxed from 0.6\n",
    "    max_features=20,\n",
    "    correlation_threshold=0.9\n",
    ")\n",
    "\n",
    "# Evaluate model\n",
    "results_unified_v2 = evaluate_unified_model(\n",
    "    dataset=dataset_unified_seeded, \n",
    "    selected_features=selected_unified_features\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
